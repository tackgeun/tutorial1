{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice2 - From Fully-Connected to Fully-Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference code\n",
    "  - https://github.com/bodokaiser/piwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine VGG-16 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16 provided by torchvision\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check that VGG16's output from feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = vgg16.features.forward(torch.zeros(1,3,224,224))\n",
    "print(feat.size(), 256*6*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. implement FCN with VGG-16 network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.conv2d\n",
    "  - https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "- view function, change tensor into different shape keeping the same number of elements\n",
    "  - https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN32(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super().__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        \n",
    "        self.feats = vgg16.features\n",
    "        \n",
    "        self.fconn = nn.Sequential(\n",
    "            nn.Conv2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        \n",
    "        # weight copy\n",
    "        self.fconn[0].weight.data = \n",
    "        self.fconn[3].weight.data = \n",
    "        \n",
    "        # bias copy\n",
    "        self.fconn[0].bias.data = \n",
    "        self.fconn[3].bias.data = \n",
    "        \n",
    "        # new score layer\n",
    "        self.score = nn.Conv2d(4096, num_classes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.feats(x)\n",
    "        #print(feats.size())\n",
    "        fconn = self.fconn(feats)\n",
    "        score = self.score(fconn)\n",
    "        upsample_t = F.upsample(score, scale_factor=32, mode='bilinear', align_corners=True)\n",
    "        return upsample_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = FCN32()\n",
    "fcn.forward(torch.zeros(1,3,512,512)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "EXTENSIONS = ['.jpg', '.png']\n",
    "\n",
    "def load_image(file):\n",
    "    return Image.open(file)\n",
    "\n",
    "def image_path(root, basename, extension):\n",
    "    return os.path.join(root, f'{basename}{extension}')\n",
    "\n",
    "\n",
    "class VOC12(Dataset):\n",
    "    def __init__(self, root, split='train', input_transform=None, target_transform=None):\n",
    "        self.images_root = os.path.join(root, 'JPEGImages')\n",
    "        self.labels_root = os.path.join(root, 'SegmentationClass')\n",
    "\n",
    "        self.filenames = []\n",
    "        with open(os.path.join(root, 'ImageSets', 'Segmentation', '%s.txt' % split)) as f:\n",
    "            for r in f.readlines():\n",
    "                self.filenames.append(r[0:-1])\n",
    "        \n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\n",
    "            image = load_image(f).convert('RGB')\n",
    "        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\n",
    "            label = load_image(f).convert('P')\n",
    "\n",
    "        if self.input_transform is not None:\n",
    "            image = self.input_transform(image)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "class Relabel:\n",
    "    def __init__(self, olabel, nlabel):\n",
    "        self.olabel = olabel\n",
    "        self.nlabel = nlabel\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        assert isinstance(tensor, torch.LongTensor), 'tensor needs to be LongTensor'\n",
    "        tensor[tensor == self.olabel] = self.nlabel\n",
    "        return tensor\n",
    "\n",
    "class ToLabel:\n",
    "    def __call__(self, image):\n",
    "        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\n",
    "\n",
    "input_transform = Compose([ \n",
    "    CenterCrop(512), \n",
    "    ToTensor(), \n",
    "    Normalize([.485, .456, .406], [.229, .224, .225]), \n",
    "])\n",
    "target_transform = Compose([ \n",
    "    CenterCrop(512), \n",
    "    ToLabel(), \n",
    "    Relabel(255, 21), # ignore label 255 >> 21\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. train a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = './VOC2012/'\n",
    "\n",
    "net = FCN32()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=21)\n",
    "\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(VOC12(DATAROOT, 'train', input_transform, target_transform),\n",
    "        num_workers=1, batch_size=10, shuffle=True)\n",
    "\n",
    "net.train()\n",
    "all_loss = 0\n",
    "for i, data in enumerate(loader):\n",
    "    image = data[0].cuda()\n",
    "    label = data[1].squeeze().cuda()\n",
    "    \n",
    "    # write code in below\n",
    "    \n",
    "    all_loss = all_loss + loss.data\n",
    "    \n",
    "    if((i % 50 == 0) or (i == len(loader)-1)):\n",
    "        print(\"[{:4d}/{:4d}] loss:{:.3f}\".format(i, len(loader),all_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(VOC12(DATAROOT, 'val', input_transform, target_transform),\n",
    "        num_workers=1, batch_size=10, shuffle=False)\n",
    "\n",
    "net.eval()\n",
    "all_loss_val = 0\n",
    "for i, data in enumerate(loader):\n",
    "    image = data[0].cuda()\n",
    "    label = data[1].squeeze().cuda()\n",
    "    \n",
    "    pred = net.forward(image)\n",
    "    loss = loss_fn(pred, label)\n",
    "    \n",
    "    all_loss_val = all_loss_val + loss.data\n",
    "    \n",
    "    if((i % 50 == 0) or (i == len(loader)-1)):\n",
    "        print(\"[{:4d}/{:4d}] loss:{:.3f}\".format(i, len(loader),all_loss_val/(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc_colormap():\n",
    "    N = 256 # number of colormap\n",
    "    VOCcolormap = np.zeros([N, 3], dtype=np.uint8)\n",
    "    for i in range(0, N):\n",
    "        (r,b,g,idx)=(0,0,0,i)\n",
    "        for j in range(0, 8):\n",
    "            r = r | ((idx & 1) << (7 - j))\n",
    "            g = g | ((idx & 2) << (7 - j))\n",
    "            b = b | ((idx & 4) << (7 - j))\n",
    "            idx = idx >> 3\n",
    "        VOCcolormap[i, :] = [r, g >> 1, b >> 2]\n",
    "    return VOCcolormap\n",
    "\n",
    "def return_pascal_segmentation(input_im):\n",
    "    VOCcolormap = get_voc_colormap()\n",
    "    im = Image.fromarray(input_im, mode='P')\n",
    "    im.putpalette(np.reshape(VOCcolormap, 768, 'C'))\n",
    "    return im\n",
    "\n",
    "\n",
    "im_idx = 0 # image index for visualization\n",
    "\n",
    "gt = data[1][im_idx,0,:,:].numpy().astype(np.uint8)\n",
    "\n",
    "val, pred_seg = pred.cpu().max(1)\n",
    "pred_seg = pred_seg[im_idx].numpy().astype(np.uint8)\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(data[0][im_idx,:,:,:].permute(1,2,0)) # C x H x W --> H x W x C\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(return_pascal_segmentation(gt))\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(return_pascal_segmentation(pred_seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. compute mean IU via confusion matrix\n",
    "- PASCAL VOC evaluation code https://github.com/npinto/VOCdevkit/blob/master/VOCcode/VOCevalseg.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mtx = torch.zeros(21,21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
